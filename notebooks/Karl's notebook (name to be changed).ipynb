{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a classifier (to be chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off lets import all the basic libraries and import all the data that Emil has engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/02-ks-projects-engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>launched</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>deadline_year</th>\n",
       "      <th>deadline_month</th>\n",
       "      <th>deadline_day</th>\n",
       "      <th>launched_year</th>\n",
       "      <th>launched_month</th>\n",
       "      <th>launched_day</th>\n",
       "      <th>launched_hour</th>\n",
       "      <th>launched_minute</th>\n",
       "      <th>name_length</th>\n",
       "      <th>name_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>October</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2015</td>\n",
       "      <td>August</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000003930</td>\n",
       "      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>failed</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>November</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2017</td>\n",
       "      <td>September</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>2013</td>\n",
       "      <td>February</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2013</td>\n",
       "      <td>January</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>2012</td>\n",
       "      <td>April</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2012</td>\n",
       "      <td>March</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>April</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2016</td>\n",
       "      <td>February</td>\n",
       "      <td>Friday</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               name  \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000003930      Greeting From Earth: ZGAC Arts Capsule For ET   \n",
       "2  1000004038                                     Where is Hank?   \n",
       "3  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "         category main_category currency    deadline             launched  \\\n",
       "0          Poetry    Publishing      GBP  2015-10-09  2015-08-11 12:12:28   \n",
       "1  Narrative Film  Film & Video      USD  2017-11-01  2017-09-02 04:43:57   \n",
       "2  Narrative Film  Film & Video      USD  2013-02-26  2013-01-12 00:20:50   \n",
       "3           Music         Music      USD  2012-04-16  2012-03-17 03:24:11   \n",
       "4     Restaurants          Food      USD  2016-04-01  2016-02-26 13:38:27   \n",
       "\n",
       "        state  backers country  ...  deadline_year  deadline_month  \\\n",
       "0      failed        0      GB  ...           2015         October   \n",
       "1      failed       15      US  ...           2017        November   \n",
       "2      failed        3      US  ...           2013        February   \n",
       "3      failed        1      US  ...           2012           April   \n",
       "4  successful      224      US  ...           2016           April   \n",
       "\n",
       "   deadline_day launched_year launched_month  launched_day launched_hour  \\\n",
       "0        Friday          2015         August       Tuesday            12   \n",
       "1     Wednesday          2017      September      Saturday             4   \n",
       "2       Tuesday          2013        January      Saturday             0   \n",
       "3        Monday          2012          March      Saturday             3   \n",
       "4        Friday          2016       February        Friday            13   \n",
       "\n",
       "  launched_minute  name_length  name_words  \n",
       "0              12           31           5  \n",
       "1              43           45           8  \n",
       "2              20           14           3  \n",
       "3              24           49           7  \n",
       "4              38           20           3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I start training a specific classifier, I want to find out the most used words from the names of the projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = dict()\n",
    "for name in data.name:\n",
    "    name = name.strip()\n",
    "    name = name.strip('[\",.-!#¤%&/()=?@£$€\\]')\n",
    "    name = name.replace(\"  \", \" \")\n",
    "    words = name.split(\" \")\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if(word in occurences):\n",
    "            occurences[word] += 1\n",
    "        else:\n",
    "            occurences[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 90469,\n",
       " '-': 46803,\n",
       " 'a': 44159,\n",
       " 'of': 32948,\n",
       " 'and': 22233,\n",
       " 'for': 20733,\n",
       " 'to': 18105,\n",
       " 'in': 15620,\n",
       " '&': 15611,\n",
       " 'new': 12151,\n",
       " 'album': 11473,\n",
       " 'film': 10094,\n",
       " 'project': 9369,\n",
       " 'by': 8837,\n",
       " 'book': 8189,\n",
       " 'your': 7964,\n",
       " 'with': 7401,\n",
       " 'game': 7094,\n",
       " 'art': 6866,\n",
       " 'on': 6435,\n",
       " 'an': 6323,\n",
       " 'music': 6154,\n",
       " 'help': 5602,\n",
       " 'first': 5565,\n",
       " 'my': 5471,\n",
       " 'from': 4914,\n",
       " 'short': 4681,\n",
       " 'you': 4550,\n",
       " 'debut': 4380,\n",
       " 'is': 4096,\n",
       " 'ep': 3847,\n",
       " 'life': 3760,\n",
       " 'series': 3632,\n",
       " 'documentary': 3616,\n",
       " 'world': 3566,\n",
       " 'at': 3476,\n",
       " 'video': 3353,\n",
       " '|': 3033,\n",
       " 'i': 3015,\n",
       " 'one': 2991,\n",
       " 'love': 2980,\n",
       " 'tour': 2852,\n",
       " 'story': 2731,\n",
       " 'novel': 2724,\n",
       " 'make': 2604,\n",
       " 'our': 2585,\n",
       " 'app': 2523,\n",
       " 'all': 2505,\n",
       " 'cd': 2461,\n",
       " 'about': 2460,\n",
       " 'show': 2358,\n",
       " '2': 2339,\n",
       " 'record': 2336,\n",
       " 'me': 2319,\n",
       " 'food': 2314,\n",
       " ':': 2294,\n",
       " '': 2230,\n",
       " 'that': 2227,\n",
       " 'it': 2198,\n",
       " 'studio': 2178,\n",
       " 'card': 2130,\n",
       " 'cards': 2127,\n",
       " 'be': 2126,\n",
       " 'festival': 2102,\n",
       " '1': 2057,\n",
       " 'get': 2029,\n",
       " 'time': 1924,\n",
       " 'up': 1892,\n",
       " 'made': 1890,\n",
       " 'feature': 1870,\n",
       " 'movie': 1819,\n",
       " '/': 1809,\n",
       " 'collection': 1792,\n",
       " 'we': 1705,\n",
       " 'magazine': 1685,\n",
       " 'clothing': 1655,\n",
       " 'home': 1649,\n",
       " 'dance': 1630,\n",
       " 'adventure': 1603,\n",
       " 'us': 1597,\n",
       " 'recording': 1593,\n",
       " 'fund': 1588,\n",
       " 'comic': 1575,\n",
       " 'kids': 1572,\n",
       " 'free': 1572,\n",
       " 'live': 1571,\n",
       " 'playing': 1570,\n",
       " 'release': 1567,\n",
       " 'edition': 1549,\n",
       " 'american': 1542,\n",
       " 'city': 1538,\n",
       " 'man': 1486,\n",
       " 'mobile': 1481,\n",
       " 'more': 1465,\n",
       " 'little': 1463,\n",
       " '3d': 1448,\n",
       " 'making': 1446,\n",
       " '+': 1422,\n",
       " 'no': 1415,\n",
       " 'dream': 1411,\n",
       " 'design': 1392,\n",
       " 'journey': 1391,\n",
       " \"children's\": 1390,\n",
       " 'full': 1386,\n",
       " 'company': 1373,\n",
       " 'black': 1372,\n",
       " \"world's\": 1368,\n",
       " 'day': 1365,\n",
       " 'photography': 1357,\n",
       " 'big': 1352,\n",
       " 'fashion': 1350,\n",
       " 'coffee': 1340,\n",
       " 'light': 1319,\n",
       " 'space': 1309,\n",
       " 'fantasy': 1308,\n",
       " 'this': 1304,\n",
       " 'best': 1299,\n",
       " 'way': 1295,\n",
       " 'play': 1289,\n",
       " 'community': 1287,\n",
       " 'de': 1286,\n",
       " 'are': 1282,\n",
       " 'back': 1253,\n",
       " 'into': 1249,\n",
       " 'out': 1242,\n",
       " 'musical': 1239,\n",
       " 'through': 1238,\n",
       " 'board': 1222,\n",
       " 'season': 1199,\n",
       " 'horror': 1188,\n",
       " 'bring': 1188,\n",
       " 'graphic': 1181,\n",
       " 'rpg': 1179,\n",
       " 'comedy': 1177,\n",
       " 'system': 1176,\n",
       " 'house': 1175,\n",
       " 'not': 1151,\n",
       " 'custom': 1149,\n",
       " 'people': 1142,\n",
       " 'launch': 1141,\n",
       " 'apparel': 1137,\n",
       " 'support': 1133,\n",
       " 'original': 1127,\n",
       " 'road': 1119,\n",
       " 'next': 1114,\n",
       " 'smart': 1114,\n",
       " 'go': 1111,\n",
       " 'vinyl': 1102,\n",
       " 'web': 1102,\n",
       " 'two': 1091,\n",
       " 'production': 1088,\n",
       " 'ultimate': 1080,\n",
       " 'great': 1079,\n",
       " 'or': 1076,\n",
       " 'last': 1068,\n",
       " 'issue': 1065,\n",
       " 'dark': 1059,\n",
       " 'family': 1052,\n",
       " 'real': 1050,\n",
       " 'how': 1047,\n",
       " 'length': 1045,\n",
       " 'la': 1039,\n",
       " 'school': 1035,\n",
       " '\"the': 1035,\n",
       " 'needs': 1034,\n",
       " 'what': 1028,\n",
       " 'high': 1025,\n",
       " '2015': 1022,\n",
       " 'theatre': 1021,\n",
       " 'stories': 1014,\n",
       " 'good': 1014,\n",
       " 'artist': 1009,\n",
       " 'can': 1009,\n",
       " 'rock': 1009,\n",
       " 'online': 1007,\n",
       " '2014': 1006,\n",
       " 'iphone': 1000,\n",
       " 'social': 999,\n",
       " 'modern': 996,\n",
       " 'truck': 991,\n",
       " 'part': 987,\n",
       " 'their': 986,\n",
       " 'photo': 983,\n",
       " 'box': 973,\n",
       " 'band': 972,\n",
       " 'volume': 969,\n",
       " 'summer': 961,\n",
       " 'guide': 956,\n",
       " '2013': 951,\n",
       " 'adventures': 948,\n",
       " 'war': 948,\n",
       " '3': 941,\n",
       " 'watch': 938,\n",
       " 'calendar': 936,\n",
       " 'night': 931,\n",
       " 'travel': 928,\n",
       " '2012': 923,\n",
       " 'dog': 921,\n",
       " 'future': 917,\n",
       " '–': 917,\n",
       " 'enamel': 914,\n",
       " 'presents': 911,\n",
       " 'print': 900,\n",
       " 'tv': 878,\n",
       " 'experience': 876,\n",
       " 'digital': 874,\n",
       " \"let's\": 874,\n",
       " 'heart': 873,\n",
       " 'america': 872,\n",
       " 'handmade': 872,\n",
       " 'fun': 865,\n",
       " 'power': 860,\n",
       " 'games': 854,\n",
       " 'party': 851,\n",
       " 'red': 851,\n",
       " 'open': 849,\n",
       " 'dead': 847,\n",
       " 'will': 847,\n",
       " 'gaming': 845,\n",
       " 'year': 842,\n",
       " 'wallet': 840,\n",
       " 'line': 837,\n",
       " 'like': 836,\n",
       " '2016': 833,\n",
       " 'save': 832,\n",
       " 'most': 829,\n",
       " 'just': 824,\n",
       " 'women': 824,\n",
       " 'leather': 822,\n",
       " 'hand': 821,\n",
       " 'as': 817,\n",
       " 'history': 815,\n",
       " 'lost': 815,\n",
       " 'christmas': 812,\n",
       " 'street': 811,\n",
       " 'farm': 810,\n",
       " 'arts': 807,\n",
       " 'reality': 807,\n",
       " 'old': 801,\n",
       " 'case': 800,\n",
       " 'water': 798,\n",
       " 'inspired': 796,\n",
       " 'zombie': 796,\n",
       " 'solo': 795,\n",
       " 'organic': 789,\n",
       " 'true': 787,\n",
       " '4': 782,\n",
       " 'living': 779,\n",
       " 'children': 779,\n",
       " 'build': 778,\n",
       " 'dice': 762,\n",
       " 'second': 761,\n",
       " \"it's\": 759,\n",
       " 'set': 755,\n",
       " 'unique': 754,\n",
       " 'urban': 750,\n",
       " 'books': 744,\n",
       " 'create': 742,\n",
       " 'do': 739,\n",
       " 'young': 739,\n",
       " 'shop': 737,\n",
       " 'ever': 737,\n",
       " 'soul': 737,\n",
       " 'exhibition': 735,\n",
       " 'better': 732,\n",
       " 'jewelry': 729,\n",
       " 'building': 726,\n",
       " 'star': 722,\n",
       " 'business': 722,\n",
       " 'natural': 722,\n",
       " 'his': 721,\n",
       " '#1': 721,\n",
       " 'girl': 717,\n",
       " 'campaign': 715,\n",
       " 'song': 714,\n",
       " 'action': 710,\n",
       " 'radio': 703,\n",
       " 'need': 700,\n",
       " 'who': 698,\n",
       " 'take': 696,\n",
       " 'cafe': 695,\n",
       " '2017': 691,\n",
       " 'brand': 690,\n",
       " 'own': 687,\n",
       " 'days': 687,\n",
       " 'songs': 686,\n",
       " 'pin': 685,\n",
       " 'green': 679,\n",
       " 'creative': 677,\n",
       " 'bag': 675,\n",
       " 'style': 673,\n",
       " '1st': 672,\n",
       " 'super': 672,\n",
       " 'her': 667,\n",
       " 'beer': 667,\n",
       " 'records': 665,\n",
       " 'dreams': 664,\n",
       " 'blue': 662,\n",
       " 'hot': 661,\n",
       " 'animated': 659,\n",
       " 'wood': 658,\n",
       " 'bike': 653,\n",
       " 'change': 650,\n",
       " 'co': 648,\n",
       " 'never': 648,\n",
       " 'pilot': 647,\n",
       " 'stand': 646,\n",
       " 'science': 646,\n",
       " 'where': 644,\n",
       " 'garden': 643,\n",
       " 'kitchen': 643,\n",
       " 'phone': 642,\n",
       " 'magic': 639,\n",
       " 'earth': 637,\n",
       " 'york': 635,\n",
       " '--': 633,\n",
       " 'cat': 632,\n",
       " 'bar': 628,\n",
       " 'deck': 626,\n",
       " 'wild': 624,\n",
       " 'beautiful': 619,\n",
       " 'west': 615,\n",
       " '(a': 613,\n",
       " 'project:': 613,\n",
       " 'local': 613,\n",
       " 't-shirt': 607,\n",
       " 'table': 607,\n",
       " 'end': 607,\n",
       " 'tales': 606,\n",
       " 'death': 606,\n",
       " 'sound': 603,\n",
       " 'car': 602,\n",
       " 'kit': 602,\n",
       " 'fire': 601,\n",
       " 'limited': 599,\n",
       " 'ice': 598,\n",
       " 'camera': 596,\n",
       " 'lp': 595,\n",
       " 'funding': 595,\n",
       " 'goes': 592,\n",
       " 'other': 590,\n",
       " 'kickstarter': 590,\n",
       " 'performance': 590,\n",
       " 'start': 588,\n",
       " '~': 584,\n",
       " 'baby': 584,\n",
       " 'brewing': 583,\n",
       " 'small': 583,\n",
       " 'gallery': 580,\n",
       " 'youth': 580,\n",
       " 'fine': 575,\n",
       " 'sci-fi': 574,\n",
       " 'hope': 573,\n",
       " 'beyond': 573,\n",
       " 'interactive': 571,\n",
       " 'pins': 571,\n",
       " 'perfect': 569,\n",
       " 'theater': 568,\n",
       " 'white': 568,\n",
       " \"don't\": 568,\n",
       " 'poetry': 567,\n",
       " 'come': 564,\n",
       " 'king': 563,\n",
       " 'nyc': 563,\n",
       " 't-shirts': 560,\n",
       " 'anthology': 559,\n",
       " 'club': 559,\n",
       " 'bringing': 559,\n",
       " 'have': 559,\n",
       " 'prints': 558,\n",
       " 'sweet': 556,\n",
       " 'only': 555,\n",
       " 'indie': 555,\n",
       " 'podcast': 553,\n",
       " 'epic': 551,\n",
       " 'media': 550,\n",
       " 'friends': 549,\n",
       " 'based': 549,\n",
       " 'fiction': 548,\n",
       " 'designs': 544,\n",
       " 'comics': 542,\n",
       " 'when': 538,\n",
       " 'beauty': 535,\n",
       " 'sports': 535,\n",
       " 'center': 535,\n",
       " 'off': 530,\n",
       " 'fringe': 530,\n",
       " 'website': 529,\n",
       " 'solar': 524,\n",
       " 'want': 524,\n",
       " 'work': 523,\n",
       " 'everyone': 521,\n",
       " 'now': 521,\n",
       " 'guitar': 520,\n",
       " 'concert': 520,\n",
       " 'revolution': 520,\n",
       " 'usa': 519,\n",
       " 'glass': 518,\n",
       " 'painting': 518,\n",
       " 'send': 517,\n",
       " 'fall': 515,\n",
       " 'tale': 514,\n",
       " 'bicycle': 514,\n",
       " 'years': 514,\n",
       " 'wall': 514,\n",
       " '5': 514,\n",
       " 'android': 512,\n",
       " 'tool': 512,\n",
       " 'coloring': 512,\n",
       " 'color': 511,\n",
       " 'independent': 510,\n",
       " 'run': 508,\n",
       " 'jazz': 506,\n",
       " 'burning': 505,\n",
       " 'park': 504,\n",
       " '100': 504,\n",
       " 'paper': 503,\n",
       " 'over': 502,\n",
       " 'bottle': 501,\n",
       " 'things': 501,\n",
       " 'easy': 499,\n",
       " 'am': 498,\n",
       " 'body': 498,\n",
       " 'girls': 496,\n",
       " 'keep': 496,\n",
       " 'ii': 494,\n",
       " 'classic': 493,\n",
       " 'men': 491,\n",
       " 'picture': 490,\n",
       " '2nd': 488,\n",
       " 'after': 488,\n",
       " 'expansion': 487,\n",
       " 'tabletop': 486,\n",
       " 'down': 486,\n",
       " 'battle': 486,\n",
       " 'luxury': 485,\n",
       " 'metal': 485,\n",
       " 'every': 485,\n",
       " 'island': 484,\n",
       " 'secret': 483,\n",
       " 'so': 481,\n",
       " 'monster': 481,\n",
       " 'simple': 480,\n",
       " 'three': 480,\n",
       " 'single': 478,\n",
       " 'culture': 477,\n",
       " 'craft': 474,\n",
       " 'artists': 473,\n",
       " 'makes': 472,\n",
       " 'electric': 470,\n",
       " 'london': 468,\n",
       " 'sauce': 468,\n",
       " 'happy': 467,\n",
       " 'wear': 467,\n",
       " 'creating': 467,\n",
       " 'moon': 465,\n",
       " 'than': 465,\n",
       " '2011': 462,\n",
       " 'education': 462,\n",
       " 'human': 461,\n",
       " 'survival': 459,\n",
       " 'publishing': 459,\n",
       " 'dragon': 458,\n",
       " 'network': 457,\n",
       " 'country': 456,\n",
       " 'learn': 455,\n",
       " 'portable': 454,\n",
       " 'ipad': 454,\n",
       " 'south': 453,\n",
       " 'god': 452,\n",
       " 'place': 450,\n",
       " 'pocket': 450,\n",
       " 'mini': 450,\n",
       " 'without': 449,\n",
       " 'room': 447,\n",
       " 'presents:': 447,\n",
       " 'market': 446,\n",
       " 'sea': 445,\n",
       " 'student': 445,\n",
       " 'full-length': 445,\n",
       " 'tea': 444,\n",
       " 'machine': 444,\n",
       " 'bakery': 443,\n",
       " 'north': 442,\n",
       " 'air': 441,\n",
       " 'pet': 441,\n",
       " 'mind': 440,\n",
       " 'wooden': 439,\n",
       " 'international': 438,\n",
       " 'personal': 438,\n",
       " 'virtual': 438,\n",
       " 'blood': 438,\n",
       " 'visual': 437,\n",
       " 'restaurant': 437,\n",
       " 'stage': 434,\n",
       " 'led': 434,\n",
       " 'printing': 434,\n",
       " 'pop': 434,\n",
       " 'platform': 433,\n",
       " 'team': 432,\n",
       " 'miniatures': 431,\n",
       " 'bbq': 431,\n",
       " 'john': 431,\n",
       " 'watches': 430,\n",
       " 'audio': 430,\n",
       " 'sustainable': 429,\n",
       " 'eyes': 429,\n",
       " 'productions': 423,\n",
       " 'final': 421,\n",
       " 'store': 420,\n",
       " 'journal': 419,\n",
       " 'age': 419,\n",
       " 'poster': 418,\n",
       " 'amazing': 418,\n",
       " 'fresh': 417,\n",
       " 'peace': 416,\n",
       " 'around': 416,\n",
       " 'fitness': 416,\n",
       " 'premium': 416,\n",
       " 'going': 415,\n",
       " 'worlds': 415,\n",
       " 'coast': 415,\n",
       " 'album:': 415,\n",
       " 'tree': 415,\n",
       " 'here': 412,\n",
       " '2.0': 412,\n",
       " 'illustrated': 412,\n",
       " 'wine': 411,\n",
       " 'movement': 411,\n",
       " 'pen': 411,\n",
       " 'robot': 411,\n",
       " 'el': 410,\n",
       " 'vegan': 410,\n",
       " 'toy': 408,\n",
       " '//': 408,\n",
       " 'heroes': 407,\n",
       " 'fundraiser': 403,\n",
       " 'learning': 402,\n",
       " 'complete': 400,\n",
       " 'stop': 400,\n",
       " 'hard': 399,\n",
       " 'shoes': 398,\n",
       " 'rise': 397,\n",
       " 'bad': 396,\n",
       " 'coming': 396,\n",
       " 'public': 396,\n",
       " 'press': 395,\n",
       " 'x': 395,\n",
       " 'long': 395,\n",
       " 'nature': 395,\n",
       " 'handcrafted': 394,\n",
       " 'generation': 393,\n",
       " 'side': 393,\n",
       " 'child': 393,\n",
       " 'shirts': 393,\n",
       " 'special': 392,\n",
       " 'quest': 390,\n",
       " 'finish': 390,\n",
       " 'again': 390,\n",
       " 'town': 389,\n",
       " 'ghost': 388,\n",
       " 'top': 388,\n",
       " 'bags': 388,\n",
       " 'webseries': 388,\n",
       " 'sun': 387,\n",
       " 'give': 386,\n",
       " 'meets': 386,\n",
       " 'sky': 385,\n",
       " 'david': 385,\n",
       " 'vol.': 385,\n",
       " 'printed': 385,\n",
       " \"i'm\": 384,\n",
       " '@': 384,\n",
       " 'healthy': 384,\n",
       " '(the': 384,\n",
       " 'album,': 383,\n",
       " 'chocolate': 382,\n",
       " 'premiere': 381,\n",
       " 'publish': 380,\n",
       " 'animal': 380,\n",
       " 'diy': 380,\n",
       " 'land': 380,\n",
       " 'vintage': 379,\n",
       " 'accessories': 379,\n",
       " 'dvd': 377,\n",
       " 'global': 377,\n",
       " 'pro': 376,\n",
       " 'join': 376,\n",
       " 'awesome': 376,\n",
       " 'river': 373,\n",
       " 'let': 372,\n",
       " 'opera': 372,\n",
       " 'shirt': 371,\n",
       " 'any': 370,\n",
       " 'mountain': 369,\n",
       " 'chronicles': 369,\n",
       " 'control': 368,\n",
       " 'thriller': 368,\n",
       " 'gourmet': 368,\n",
       " 'development': 367,\n",
       " 'products': 367,\n",
       " 'technology': 365,\n",
       " 'grow': 364,\n",
       " 'smartphone': 362,\n",
       " 'together': 362,\n",
       " 'money': 362,\n",
       " 'football': 361,\n",
       " 'gold': 361,\n",
       " 'cookbook': 360,\n",
       " 'trip': 360,\n",
       " '—': 359,\n",
       " 'spring': 357,\n",
       " 'hero': 357,\n",
       " '6': 357,\n",
       " '10': 356,\n",
       " 'motion': 356,\n",
       " 'wars': 355,\n",
       " 'look': 355,\n",
       " 'portraits': 354,\n",
       " 'dogs': 354,\n",
       " 'chicago': 353,\n",
       " '100%': 353,\n",
       " 'co.': 353,\n",
       " 'pizza': 350,\n",
       " 'find': 350,\n",
       " 'national': 350,\n",
       " 'san': 349,\n",
       " 'quality': 349,\n",
       " 'golf': 349,\n",
       " 'under': 349,\n",
       " 'thesis': 348,\n",
       " 'paintings': 347,\n",
       " 'ios': 346,\n",
       " 'collective': 346,\n",
       " 'freedom': 345,\n",
       " 'boy': 345,\n",
       " 'affordable': 345,\n",
       " 'see': 344,\n",
       " 'los': 343,\n",
       " '#2': 342,\n",
       " 'fan': 342,\n",
       " 'too': 342,\n",
       " 'before': 342,\n",
       " '7': 341,\n",
       " 'lives': 341,\n",
       " 'self': 340,\n",
       " 'legend': 340,\n",
       " 'know': 340,\n",
       " 'double': 339,\n",
       " 'state': 338,\n",
       " 'college': 338,\n",
       " 'worship': 338,\n",
       " 'funds': 337,\n",
       " 'device': 337,\n",
       " 'east': 337,\n",
       " 'voice': 337,\n",
       " 'event': 337,\n",
       " 'truth': 337,\n",
       " 'southern': 336,\n",
       " 'die': 336,\n",
       " 'search': 336,\n",
       " 'spirit': 336,\n",
       " 'winter': 335,\n",
       " 'apple': 335,\n",
       " 'key': 335,\n",
       " 'gift': 334,\n",
       " 'designed': 333,\n",
       " 'writing': 332,\n",
       " 'cooking': 331,\n",
       " 'works': 331,\n",
       " 'words': 331,\n",
       " 'training': 331,\n",
       " 'videos': 331,\n",
       " 'call': 330,\n",
       " 'turn': 330,\n",
       " 'studios': 330,\n",
       " 'drive': 330,\n",
       " 'everything': 330,\n",
       " 'michael': 329,\n",
       " 'health': 329,\n",
       " 'residency': 328,\n",
       " 'portrait': 328,\n",
       " 'escape': 328,\n",
       " 'universal': 327,\n",
       " 'christian': 327,\n",
       " 'behind': 327,\n",
       " 'there': 326,\n",
       " 'share': 326,\n",
       " 'cover': 326,\n",
       " 'james': 326,\n",
       " 'episode': 324,\n",
       " 'pi': 324,\n",
       " 'source': 322,\n",
       " 'steampunk': 321,\n",
       " 'wireless': 321,\n",
       " 'en': 321,\n",
       " 'annual': 319,\n",
       " 'n': 318,\n",
       " 'cool': 318,\n",
       " 'sculpture': 317,\n",
       " 'post': 317,\n",
       " 'tiny': 316,\n",
       " 'four': 314,\n",
       " 'program': 314,\n",
       " 'trailer': 313,\n",
       " 'planet': 313,\n",
       " 'growing': 312,\n",
       " 'broken': 311,\n",
       " 'anniversary': 310,\n",
       " 'van': 309,\n",
       " 'return': 309,\n",
       " 'internet': 309,\n",
       " 'gear': 308,\n",
       " 'holder': 308,\n",
       " 'inside': 307,\n",
       " 'le': 307,\n",
       " 'fly': 306,\n",
       " 'word': 306,\n",
       " 'challenge': 306,\n",
       " 'solution': 306,\n",
       " 'face': 305,\n",
       " 'edinburgh': 303,\n",
       " 'minimalist': 303,\n",
       " 'service': 303,\n",
       " 'news': 303,\n",
       " 'printer': 302,\n",
       " 'museum': 302,\n",
       " 'fight': 302,\n",
       " 'bear': 301,\n",
       " 'kind': 301,\n",
       " 'safe': 301,\n",
       " 'kickstart': 301,\n",
       " 'changing': 301,\n",
       " 'mystery': 301,\n",
       " 'modular': 301,\n",
       " 'workshop': 301,\n",
       " 'innovative': 301,\n",
       " 'legacy': 300,\n",
       " 'everyday': 300,\n",
       " 'entertainment': 300,\n",
       " 'five': 300,\n",
       " 'educational': 299,\n",
       " 'beach': 299,\n",
       " 'safety': 299,\n",
       " 'youtube': 299,\n",
       " 'across': 299,\n",
       " 'vs': 297,\n",
       " 'care': 297,\n",
       " 'ball': 297,\n",
       " 'lights': 297,\n",
       " 'uk': 295,\n",
       " 'energy': 295,\n",
       " 'eat': 294,\n",
       " 'stickers': 294,\n",
       " 'photographic': 294,\n",
       " 'magnetic': 294,\n",
       " 'tribute': 294,\n",
       " 'blues': 293,\n",
       " 'w/': 293,\n",
       " 'right': 292,\n",
       " 'some': 292,\n",
       " 'was': 291,\n",
       " 'between': 291,\n",
       " 'please': 290,\n",
       " 'ride': 290,\n",
       " 'titanium': 289,\n",
       " 'screen': 289,\n",
       " 'backpack': 288,\n",
       " 'level': 288,\n",
       " 'yoga': 287,\n",
       " 'ready': 287,\n",
       " 'if': 286,\n",
       " 'retro': 286,\n",
       " 'detroit': 285,\n",
       " 'mural': 285,\n",
       " 'name': 284,\n",
       " 'meet': 284,\n",
       " 'stars': 284,\n",
       " 'revolutionary': 283,\n",
       " 'hop': 283,\n",
       " 'wants': 283,\n",
       " 'camp': 283,\n",
       " 'creations': 282,\n",
       " 'installation': 281,\n",
       " 'piano': 281,\n",
       " 'pack': 281,\n",
       " 'has': 280,\n",
       " 'mr.': 280,\n",
       " 'finding': 279,\n",
       " 'professional': 279,\n",
       " 'monsters': 278,\n",
       " 'saving': 277,\n",
       " 'golden': 277,\n",
       " 'hands': 277,\n",
       " 'library': 277,\n",
       " '2:': 276,\n",
       " 'hip': 276,\n",
       " 'drama': 276,\n",
       " 'oil': 276,\n",
       " '=': 275,\n",
       " 'dungeon': 275,\n",
       " 'strategy': 274,\n",
       " 'puzzle': 274,\n",
       " 'texas': 273,\n",
       " 'map': 272,\n",
       " 'cream': 272,\n",
       " 'another': 271,\n",
       " 'carbon': 271,\n",
       " 'designer': 270,\n",
       " 'official': 270,\n",
       " 'woman': 269,\n",
       " 'clean': 269,\n",
       " 'hidden': 269,\n",
       " \"'the\": 269,\n",
       " 'eye': 268,\n",
       " 'station': 268,\n",
       " 'lifestyle': 268,\n",
       " 'rose': 267,\n",
       " 'tarot': 267,\n",
       " 'contemporary': 266,\n",
       " 'cake': 266,\n",
       " 'brewery': 265,\n",
       " 'paint': 265,\n",
       " 'italian': 265,\n",
       " 'being': 265,\n",
       " 'raspberry': 265,\n",
       " 'put': 264,\n",
       " 'holiday': 264,\n",
       " 'stone': 264,\n",
       " 'fest': 264,\n",
       " 'hour': 264,\n",
       " 'hair': 264,\n",
       " 'adult': 263,\n",
       " 'cold': 263,\n",
       " 'talk': 262,\n",
       " 'plush': 262,\n",
       " 'century': 262,\n",
       " 'roll': 262,\n",
       " 'but': 261,\n",
       " 'arduino': 261,\n",
       " 'got': 261,\n",
       " 'its': 261,\n",
       " 'students': 261,\n",
       " 'something': 261,\n",
       " 'zero': 260,\n",
       " 'films': 260,\n",
       " 'deep': 260,\n",
       " 'head': 260,\n",
       " 'legends': 259,\n",
       " 'phase': 259,\n",
       " 'universe': 259,\n",
       " 'laser': 258,\n",
       " 'group': 258,\n",
       " 'brooklyn': 258,\n",
       " 'furniture': 258,\n",
       " 'shadow': 258,\n",
       " 'mission': 258,\n",
       " 'forever': 257,\n",
       " 'jack': 257,\n",
       " 'vision': 256,\n",
       " 'edge': 256,\n",
       " 'usb': 256,\n",
       " 'mark': 256,\n",
       " 'cup': 255,\n",
       " 'valley': 255,\n",
       " 'why': 255,\n",
       " 'kid': 255,\n",
       " 'fish': 254,\n",
       " 'toys': 254,\n",
       " 'california': 254,\n",
       " 'steel': 253,\n",
       " 'week': 253,\n",
       " 'book:': 253,\n",
       " 'creation': 252,\n",
       " 'equipment': 252,\n",
       " 'animation': 251,\n",
       " 'mount': 251,\n",
       " 'candles': 251,\n",
       " 'startup': 250,\n",
       " 'planner': 250,\n",
       " 'angel': 250,\n",
       " 'cancer': 250,\n",
       " 'apocalypse': 249,\n",
       " 'orchestra': 249,\n",
       " 'fair': 248,\n",
       " 'friendly': 248,\n",
       " 'memory': 247,\n",
       " 'race': 247,\n",
       " 'still': 247,\n",
       " 'silver': 247,\n",
       " 'fit': 246,\n",
       " 'cats': 246,\n",
       " 'label': 246,\n",
       " 'against': 246,\n",
       " 'hollywood': 246,\n",
       " 'master': 246,\n",
       " 'society': 246,\n",
       " 'door': 245,\n",
       " 'kingdom': 245,\n",
       " 'con': 245,\n",
       " 'dirty': 245,\n",
       " 'fishing': 244,\n",
       " 'finishing': 243,\n",
       " 'piece': 243,\n",
       " 'walk': 243,\n",
       " 'photos': 243,\n",
       " 'charger': 243,\n",
       " 'fast': 242,\n",
       " 'honey': 242,\n",
       " 'forgotten': 241,\n",
       " 'powered': 241,\n",
       " 'become': 241,\n",
       " 'outdoor': 241,\n",
       " 'hell': 241,\n",
       " 'trail': 241,\n",
       " 'chris': 240,\n",
       " \"women's\": 239,\n",
       " 'different': 239,\n",
       " 'language': 239,\n",
       " '30': 238,\n",
       " 'artisan': 238,\n",
       " 'touch': 238,\n",
       " 'boys': 237,\n",
       " 'grand': 237,\n",
       " 'queen': 237,\n",
       " 'seven': 236,\n",
       " 'brothers': 236,\n",
       " 'exhibit': 236,\n",
       " 'automatic': 236,\n",
       " 'gay': 236,\n",
       " 'llc': 235,\n",
       " 'they': 235,\n",
       " 'sleep': 235,\n",
       " 'crazy': 235,\n",
       " 'stay': 235,\n",
       " 'healing': 234,\n",
       " 'ring': 233,\n",
       " 'away': 233,\n",
       " 'title': 233,\n",
       " 'software': 233,\n",
       " 'produce': 232,\n",
       " 'english': 232,\n",
       " 'coin': 232,\n",
       " 'nation': 232,\n",
       " 'chicken': 231,\n",
       " 'skin': 231,\n",
       " 'vs.': 231,\n",
       " 'computer': 230,\n",
       " 'channel': 230,\n",
       " 'twist': 230,\n",
       " 'halloween': 229,\n",
       " 'animals': 229,\n",
       " 'alive': 229,\n",
       " 'memoir': 229,\n",
       " 'third': 229,\n",
       " 'fairy': 228,\n",
       " 'rising': 228,\n",
       " 'senior': 228,\n",
       " 'projects': 228,\n",
       " 'getting': 228,\n",
       " 'friend': 228,\n",
       " 'dress': 228,\n",
       " 'chapter': 228,\n",
       " 'zine': 227,\n",
       " '3rd': 227,\n",
       " 'ink': 227,\n",
       " 'evolution': 227,\n",
       " 'pc': 227,\n",
       " 'helping': 227,\n",
       " '8': 226,\n",
       " 'boutique': 226,\n",
       " 'tell': 226,\n",
       " 'fiber': 226,\n",
       " 'concept': 226,\n",
       " 'candy': 226,\n",
       " 'field': 226,\n",
       " 'ancient': 226,\n",
       " 'step': 225,\n",
       " 'awareness': 225,\n",
       " '12': 224,\n",
       " 'lake': 224,\n",
       " 'very': 224,\n",
       " 'class': 224,\n",
       " '50': 224,\n",
       " 'trilogy': 223,\n",
       " 'looking': 223,\n",
       " 'lady': 223,\n",
       " 'electronic': 223,\n",
       " 'wearable': 222,\n",
       " 'soap': 222,\n",
       " 'united': 222,\n",
       " 'point': 221,\n",
       " 'mother': 221,\n",
       " 'africa': 220,\n",
       " 'cheese': 220,\n",
       " 'bluetooth': 220,\n",
       " 'welcome': 220,\n",
       " 'greatest': 220,\n",
       " 'chef': 220,\n",
       " 'cloud': 219,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(occurences.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329421"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, nearly only third of the projects contain the word \"the\", which is a lot less than I thought.\n",
    "However my first approach to train a classifier is by creating binary fields whether the title included specific word or not and create those kind of fields for top 20 words, from which roughly first 10 of them are conjunctions or articles and the last 10 have already a bit more meaning in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "-\n",
      "a\n",
      "of\n",
      "and\n",
      "for\n",
      "to\n",
      "in\n",
      "&\n",
      "new\n",
      "album\n",
      "film\n",
      "project\n",
      "by\n",
      "book\n",
      "your\n",
      "with\n",
      "game\n",
      "art\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for (key,value) in sorted(occurences.items(), key=lambda item: item[1], reverse=True):\n",
    "    doesOccure = []\n",
    "    print(key)\n",
    "    for title in data.name:\n",
    "        if(key in title.lower()):\n",
    "            doesOccure.append(1)\n",
    "        else:\n",
    "            doesOccure.append(0)\n",
    "    data.insert(len(data.columns),\"Title Contains \"+key, doesOccure, True)\n",
    "    if(i == 19):\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>launched</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>Title Contains album</th>\n",
       "      <th>Title Contains film</th>\n",
       "      <th>Title Contains project</th>\n",
       "      <th>Title Contains by</th>\n",
       "      <th>Title Contains book</th>\n",
       "      <th>Title Contains your</th>\n",
       "      <th>Title Contains with</th>\n",
       "      <th>Title Contains game</th>\n",
       "      <th>Title Contains art</th>\n",
       "      <th>Title Contains on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000003930</td>\n",
       "      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>failed</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               name  \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000003930      Greeting From Earth: ZGAC Arts Capsule For ET   \n",
       "2  1000004038                                     Where is Hank?   \n",
       "3  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "         category main_category currency    deadline             launched  \\\n",
       "0          Poetry    Publishing      GBP  2015-10-09  2015-08-11 12:12:28   \n",
       "1  Narrative Film  Film & Video      USD  2017-11-01  2017-09-02 04:43:57   \n",
       "2  Narrative Film  Film & Video      USD  2013-02-26  2013-01-12 00:20:50   \n",
       "3           Music         Music      USD  2012-04-16  2012-03-17 03:24:11   \n",
       "4     Restaurants          Food      USD  2016-04-01  2016-02-26 13:38:27   \n",
       "\n",
       "        state  backers country  ...  Title Contains album  \\\n",
       "0      failed        0      GB  ...                     0   \n",
       "1      failed       15      US  ...                     0   \n",
       "2      failed        3      US  ...                     0   \n",
       "3      failed        1      US  ...                     1   \n",
       "4  successful      224      US  ...                     0   \n",
       "\n",
       "   Title Contains film  Title Contains project Title Contains by  \\\n",
       "0                    0                       0                 0   \n",
       "1                    0                       0                 0   \n",
       "2                    0                       0                 0   \n",
       "3                    0                       0                 0   \n",
       "4                    0                       0                 0   \n",
       "\n",
       "  Title Contains book  Title Contains your Title Contains with  \\\n",
       "0                   0                    0                   0   \n",
       "1                   0                    0                   0   \n",
       "2                   0                    0                   0   \n",
       "3                   0                    0                   0   \n",
       "4                   0                    0                   0   \n",
       "\n",
       "  Title Contains game  Title Contains art  Title Contains on  \n",
       "0                   0                   0                  1  \n",
       "1                   0                   1                  0  \n",
       "2                   0                   0                  0  \n",
       "3                   0                   0                  0  \n",
       "4                   0                   0                  1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be working, so now its time to delete all of the cells that we did not create (except for usd_goal_rea) and see how we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'name', 'category', 'main_category', 'currency', 'deadline',\n",
      "       'launched', 'state', 'backers', 'country', 'usd_pledged_real',\n",
      "       'usd_goal_real', 'deadline_year', 'deadline_month', 'deadline_day',\n",
      "       'launched_year', 'launched_month', 'launched_day', 'launched_hour',\n",
      "       'launched_minute', 'name_length', 'name_words', 'Title Contains the',\n",
      "       'Title Contains -', 'Title Contains a', 'Title Contains of',\n",
      "       'Title Contains and', 'Title Contains for', 'Title Contains to',\n",
      "       'Title Contains in', 'Title Contains &', 'Title Contains new',\n",
      "       'Title Contains album', 'Title Contains film', 'Title Contains project',\n",
      "       'Title Contains by', 'Title Contains book', 'Title Contains your',\n",
      "       'Title Contains with', 'Title Contains game', 'Title Contains art',\n",
      "       'Title Contains on'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data = data.drop(columns= [\"name\",\"launched\",\"deadline\",\"usd_pledged_real\",\"backers\",\"category\",\"country\",\"main_category\",\"currency\",\"deadline_month\",\"deadline_day\",\"deadline_year\",\"launched_year\",\"launched_month\",\"launched_day\",\"launched_hour\",\"launched_minute\"])\n",
    "data.state = data.state.map(dict(failed=0,successful=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the data up for training and testing (with random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "X = X.drop(columns=[\"state\"])\n",
    "y= data[\"state\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learned this from the internet!\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1600, num = 200)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 4, 6, 8, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 46.1min remaining: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 56.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(max_depth=4),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 15, 20, 25, 31, 36,\n",
       "                                                      41, 46, 52, 57, 62, 67,\n",
       "                                                      73, 78, 83, 88, 94, 99,\n",
       "                                                      104, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                                        'n_estimators': [200, 207, 214, 221,\n",
       "                                                         228, 235, 242, 249,\n",
       "                                                         256, 263, 270, 277,\n",
       "                                                         284, 291, 298, 305,\n",
       "                                                         312, 319, 326, 333,\n",
       "                                                         340, 347, 354, 361,\n",
       "                                                         368, 375, 382, 389,\n",
       "                                                         396, 404, ...]},\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I googled a way to improve model with multiple threads.\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,n_iter = 10, cv = 3, verbose=5, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "\n",
    "\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_random.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6340069009481215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'usd_goal_real', 'name_length', 'name_words',\n",
       "       'Title Contains the', 'Title Contains -', 'Title Contains a',\n",
       "       'Title Contains of', 'Title Contains and', 'Title Contains for',\n",
       "       'Title Contains to', 'Title Contains in', 'Title Contains &',\n",
       "       'Title Contains new', 'Title Contains album', 'Title Contains film',\n",
       "       'Title Contains project', 'Title Contains by', 'Title Contains book',\n",
       "       'Title Contains your', 'Title Contains with', 'Title Contains game',\n",
       "       'Title Contains art', 'Title Contains on'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = 0\n",
    "false = 0\n",
    "i = 0\n",
    "for result in y_test:\n",
    "    if(result == prediction[i]):\n",
    "        right += 1\n",
    "    else:\n",
    "        false += 1\n",
    "    i += 1\n",
    "\n",
    "print(right/(right+false))\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, we can predict with 63% accuracy based on the goal was set and what of the top 20 words the titles contain, which isn't anything special.\n",
    "Now I'm going to try it all over again, but this time leave all the other data in and  use only half of the training data (because otherwise it would easily go over 24 hours to train it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/02-ks-projects-engineered.csv')\n",
    "\n",
    "\n",
    "i = 0\n",
    "for (key,value) in sorted(occurences.items(), key=lambda item: item[1], reverse=True):\n",
    "    doesOccure = []\n",
    "    for title in data.name:\n",
    "        if(key in title.lower()):\n",
    "            doesOccure.append(1)\n",
    "        else:\n",
    "            doesOccure.append(0)\n",
    "    data.insert(len(data.columns),\"Title Contains \"+key, doesOccure, True)\n",
    "    if(i == 19):\n",
    "        break\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "data = data.drop(columns= [\"name\",\"launched\",\"deadline\",\"usd_pledged_real\",\"backers\",\"category\"])\n",
    "data.state = data.state.map(dict(failed=0,successful=1))\n",
    "data.launched_month = data.launched_month.map(dict(January=0,February=1,March=2,April=3,May=4,June=5,July=6,August=7,September=8,October=9,November=10,December=11))\n",
    "data.deadline_month = data.deadline_month.map(dict(January=0,February=1,March=2,April=3,May=4,June=5,July=6,August=7,September=8,October=9,November=10,December=11))\n",
    "data.launched_day = data.launched_day.map(dict(Monday=0,Tuesday=1,Thursday=2,Wednesday=3,Friday=4,Saturday=5,Sunday=6))\n",
    "data.deadline_day = data.deadline_day.map(dict(Monday=0,Tuesday=1,Thursday=2,Wednesday=3,Friday=4,Saturday=5,Sunday=6))\n",
    "#I turn the months and weekdays to numbers so we could save a lot of columns. The result might vary a bit, but not much.\n",
    "data_dum = pd.get_dummies(data,columns=[\"main_category\",\"currency\",\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_dum.copy()\n",
    "X = X.drop(columns=[\"state\"])\n",
    "y= data_dum[\"state\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)\n",
    "X_trainHalf, X_another, y_trainHalf, y_another = train_test_split(X_train, y_train, train_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we split up the data one more time so we would use just half of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 24.6min remaining:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 34.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(max_depth=4),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 15, 20, 25, 31, 36,\n",
       "                                                      41, 46, 52, 57, 62, 67,\n",
       "                                                      73, 78, 83, 88, 94, 99,\n",
       "                                                      104, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                                        'n_estimators': [200, 207, 214, 221,\n",
       "                                                         228, 235, 242, 249,\n",
       "                                                         256, 263, 270, 277,\n",
       "                                                         284, 291, 298, 305,\n",
       "                                                         312, 319, 326, 333,\n",
       "                                                         340, 347, 354, 361,\n",
       "                                                         368, 375, 382, 389,\n",
       "                                                         396, 404, ...]},\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomNew = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,n_iter = 10, cv = 2, verbose=5, n_jobs = -1)\n",
    "rf_randomNew.fit(X_trainHalf, y_trainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675068554140063\n"
     ]
    }
   ],
   "source": [
    "prediction2 = rf_randomNew.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "right = 0\n",
    "false = 0\n",
    "i = 0\n",
    "for result in y_test:\n",
    "    if(result == prediction2[i]):\n",
    "        right += 1\n",
    "    else:\n",
    "        false += 1\n",
    "    i += 1\n",
    "\n",
    "print(right/(right+false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67.5% accuracy is pretty similar to our last classifier and while its still better than nothing, I would count it rather low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next test I'm trying once again to predict based on the title only, but this time we use method described in this article https://stackabuse.com/text-classification-with-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/02-ks-projects-engineered.csv')\n",
    "\n",
    "X = data[\"name\"].copy()\n",
    "y = data[\"state\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karla431\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karla431\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for title in X:\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(title))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)\n",
    "X_trainHalf, X_another, y_trainHalf, y_another = train_test_split(X_train, y_train, train_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 262.8min remaining: 95.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 355.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(max_depth=4),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 15, 20, 25, 31, 36,\n",
       "                                                      41, 46, 52, 57, 62, 67,\n",
       "                                                      73, 78, 83, 88, 94, 99,\n",
       "                                                      104, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                                        'n_estimators': [200, 207, 214, 221,\n",
       "                                                         228, 235, 242, 249,\n",
       "                                                         256, 263, 270, 277,\n",
       "                                                         284, 291, 298, 305,\n",
       "                                                         312, 319, 326, 333,\n",
       "                                                         340, 347, 354, 361,\n",
       "                                                         368, 375, 382, 389,\n",
       "                                                         396, 404, ...]},\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I googled a way to improve model with multiple threads.\n",
    "rf_randomWords = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,n_iter = 10, cv = 3, verbose=5, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "\n",
    "\n",
    "rf_randomWords.fit(X_trainHalf, y_trainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6450463942040131\n"
     ]
    }
   ],
   "source": [
    "prediction3 = rf_randomWords.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "right = 0\n",
    "false = 0\n",
    "i = 0\n",
    "for result in y_test:\n",
    "    if(result == prediction3[i]):\n",
    "        right += 1\n",
    "    else:\n",
    "        false += 1\n",
    "    i += 1\n",
    "\n",
    "print(right/(right+false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65% is not bad score at all since the prediction used the title only. Lets try to combine this data back together with our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/02-ks-projects-engineered.csv')\n",
    "\n",
    "    \n",
    "data = data.drop(columns= [\"name\",\"launched\",\"deadline\",\"usd_pledged_real\",\"backers\",\"category\"])\n",
    "data.state = data.state.map(dict(failed=0,successful=1))\n",
    "data.launched_month = data.launched_month.map(dict(January=0,February=1,March=2,April=3,May=4,June=5,July=6,August=7,September=8,October=9,November=10,December=11))\n",
    "data.deadline_month = data.deadline_month.map(dict(January=0,February=1,March=2,April=3,May=4,June=5,July=6,August=7,September=8,October=9,November=10,December=11))\n",
    "data.launched_day = data.launched_day.map(dict(Monday=0,Tuesday=1,Thursday=2,Wednesday=3,Friday=4,Saturday=5,Sunday=6))\n",
    "data.deadline_day = data.deadline_day.map(dict(Monday=0,Tuesday=1,Thursday=2,Wednesday=3,Friday=4,Saturday=5,Sunday=6))\n",
    "#I turn the months and weekdays to numbers so we could save a lot of columns. The result might vary a bit, but not much.\n",
    "data_dum = pd.get_dummies(data,columns=[\"main_category\",\"currency\",\"country\"])\n",
    "\n",
    "for column in range(1500):\n",
    "    newColumn = [] \n",
    "    for row in range(len(X)):\n",
    "        newColumn.append(X[row][column])\n",
    "    data_dum.insert(len(data_dum.columns),\"Title\"+str(i), newColumn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dum.columns\n",
    "X = data_dum.copy()\n",
    "X = X.drop(columns=[\"state\"])\n",
    "y= data_dum[\"state\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)\n",
    "X_trainHalf, X_another, y_trainHalf, y_another = train_test_split(X_train, y_train, train_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 53.6min remaining: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 113.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(max_depth=4),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 15, 20, 25, 31, 36,\n",
       "                                                      41, 46, 52, 57, 62, 67,\n",
       "                                                      73, 78, 83, 88, 94, 99,\n",
       "                                                      104, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                                        'n_estimators': [200, 207, 214, 221,\n",
       "                                                         228, 235, 242, 249,\n",
       "                                                         256, 263, 270, 277,\n",
       "                                                         284, 291, 298, 305,\n",
       "                                                         312, 319, 326, 333,\n",
       "                                                         340, 347, 354, 361,\n",
       "                                                         368, 375, 382, 389,\n",
       "                                                         396, 404, ...]},\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I googled a way to improve model with multiple threads.\n",
    "rf_randomWordsWithOthers = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,n_iter = 10, cv = 3, verbose=5, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "\n",
    "\n",
    "rf_randomWordsWithOthers.fit(X_trainHalf, y_trainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "prediction4 = rf_randomWordsWithOthers.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "#This 0.0 was a mistake in the code, which now I moved to the next cell and reran with fixes. (I don't want to run the prediction again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6865229137786233\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "false = 0\n",
    "i = 0\n",
    "for result in y_test:\n",
    "    if(result == prediction4[i]):\n",
    "        right += 1\n",
    "    else:\n",
    "        false += 1\n",
    "    i += 1\n",
    "\n",
    "print(right/(right+false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I conclusion we can say that getting over 70% prediction rate is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
